{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GANs_Faces.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNlPlile4J2KcjiMvKqvwZk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ms0017/GANs_exploration/blob/master/GANs_Faces.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OQrYT6MnGwcF",
        "outputId": "346ea70f-c364-4e96-a690-4e37625f93c1"
      },
      "source": [
        "# Mount G-Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ztL_1aZ3HKv2",
        "outputId": "6f289a23-a4cc-4642-d2cf-9bf61ac53ee3"
      },
      "source": [
        "#Next, clone StyleGAN2 ADA PyTorch from GitHub.\n",
        "!git clone https://github.com/NVlabs/stylegan2-ada-pytorch.git\n",
        "!pip install ninja"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'stylegan2-ada-pytorch' already exists and is not an empty directory.\n",
            "Requirement already satisfied: ninja in /usr/local/lib/python3.7/dist-packages (1.10.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sEwBXVHgHKsH",
        "outputId": "968a4d85-6f91-4458-d749-21a354532106"
      },
      "source": [
        "#Verify that StyleGAN has been cloned.\n",
        "!ls /content/stylegan2-ada-pytorch/"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "calc_metrics.py  docker_run.sh\tLICENSE.txt   README.md        train.py\n",
            "dataset_tool.py  docs\t\tmetrics       style_mixing.py\n",
            "dnnlib\t\t generate.py\tprojector.py  torch_utils\n",
            "Dockerfile\t legacy.py\t__pycache__   training\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L6NRjqc-HKy_"
      },
      "source": [
        "import sys\n",
        "sys.path.insert(0, \"/content/stylegan2-ada-pytorch\")\n",
        "import pickle\n",
        "import os\n",
        "import numpy as np\n",
        "import PIL.Image\n",
        "from IPython.display import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import IPython.display\n",
        "import torch\n",
        "import dnnlib\n",
        "import legacy\n",
        "from tqdm.notebook import tqdm"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8kXCUA5SgEio"
      },
      "source": [
        "# takes short seed numbers \"seed\" eg 6600 and expand them into a 512 latent vector, which is a floating point value\n",
        "# The seed value is easier to represent in code than a 512 value vector. However, while a small change to the latent vector \n",
        "# results in a small change to the image, even a small change to the seed value will produce a radically different image.\n",
        "# Can think of the latent vector as the DNA\n",
        "def seed2vec(G, seed):\n",
        "  return np.random.RandomState(seed).randn(1, G.z_dim)"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V2fUTlITgE63"
      },
      "source": [
        "def display_image(image):\n",
        "  plt.axis('off')\n",
        "  plt.imshow(image)\n",
        "  plt.show()"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "or-Gr2SogFNe"
      },
      "source": [
        "# G: Neural Net\n",
        "# z: 512 lantent vectors/ seeds\n",
        "# truncation_psi: used by stylegan2 to make images clearer\n",
        "def generate_image(G, z, truncation_psi):\n",
        "    # Render images for dlatents initialized from random seeds.\n",
        "    Gs_kwargs = {\n",
        "        'output_transform': dict(func=tflib.convert_images_to_uint8, nchw_to_nhwc=True),\n",
        "        'randomize_noise': False\n",
        "    }\n",
        "    if truncation_psi is not None:\n",
        "        Gs_kwargs['truncation_psi'] = truncation_psi\n",
        "\n",
        "    label = np.zeros([1] + G.input_shapes[1][1:])\n",
        "    images = G.run(z, label, **G_kwargs) # [minibatch, height, width, channel]\n",
        "    return images[0]"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ltl2EpdOgFtO"
      },
      "source": [
        "def get_label(G, device, class_idx):\n",
        "  label = torch.zeros([1, G.c_dim], device=device)\n",
        "  if G.c_dim != 0:\n",
        "      if class_idx is None:\n",
        "          ctx.fail('Must specify class label with --class when using a conditional network')\n",
        "      label[:, class_idx] = 1\n",
        "  else:\n",
        "      if class_idx is not None:\n",
        "          print ('warn: --class=lbl ignored when running on an unconditional network')\n",
        "  return label"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SuNXe62agGDX"
      },
      "source": [
        "def generate_image(device, G, z, truncation_psi=1.0, noise_mode='const', class_idx=None):\n",
        "  z = torch.from_numpy(z).to(device)\n",
        "  label = get_label(G, device, class_idx)\n",
        "  img = G(z, label, truncation_psi=truncation_psi, noise_mode=noise_mode)\n",
        "  img = (img.permute(0, 2, 3, 1) * 127.5 + 128).clamp(0, 255).to(torch.uint8)\n",
        "  #PIL.Image.fromarray(img[0].cpu().numpy(), 'RGB').save(f'{outdir}/seed{seed:04d}.png')\n",
        "  return PIL.Image.fromarray(img[0].cpu().numpy(), 'RGB')"
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rBPk2JP6gHEV"
      },
      "source": [
        "def expand_seed(seeds, vector_size):\n",
        "  result = []\n",
        "  for seed in seeds:\n",
        "    rnd = np.random.RandomState(seed)\n",
        "    result.append( rnd.randn(1, vector_size) ) \n",
        "  return result"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zAhtcP0kgHbA"
      },
      "source": [
        "def download_torch_pretrained_instance(device):\n",
        "  #URL = \"https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada/pretrained/ffhq.pkl\"\n",
        "  URL = \"https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/ffhq.pkl\"\n",
        "  print(f'Loading networks from \"{URL}\"...')\n",
        "  with dnnlib.util.open_url(URL) as f:\n",
        "      G = legacy.load_network_pkl(f)['G_ema'].to(device) # type: ignore\n",
        "  return G"
      ],
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q4gTZEaJgH2C"
      },
      "source": [
        "def get_images(G, device):\n",
        "  # Choose your own starting and ending seed.\n",
        "  SEED_FROM = 2000\n",
        "  SEED_TO = 2003\n",
        "\n",
        "  # Generate the images for the seeds.\n",
        "  for i in range(SEED_FROM, SEED_TO):\n",
        "    print(f\"Seed {i}\")\n",
        "    z = seed2vec(G, i)\n",
        "    img = generate_image(device, G, z)\n",
        "    display_image(img)"
      ],
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YStpAn4EgIYd"
      },
      "source": [
        "def explore_seed(G):\n",
        "  vector_size = G.z_dim\n",
        "  # range(8192,8300)\n",
        "  seeds = expand_seed( [8192+1,8192+9], vector_size)\n",
        "  #generate_images(Gs, seeds,truncation_psi=0.5)\n",
        "  print(seeds[0].shape)"
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2IHKBxDjgJJ2"
      },
      "source": [
        "def morph_video(G, device):\n",
        "  # Choose your seeds to morph through and the \n",
        "  # number of steps to take to get to each.\n",
        "  SEEDS = [2000,2003,2001]\n",
        "  STEPS = 100\n",
        "\n",
        "  # Remove any prior results\n",
        "  !rm /content/results/* \n",
        "  os.makedirs(\"./results/\", exist_ok=True)\n",
        "\n",
        "  # Generate the images for the video.\n",
        "  idx = 0\n",
        "  for i in range(len(SEEDS)-1):\n",
        "    v1 = seed2vec(G, SEEDS[i])\n",
        "    v2 = seed2vec(G, SEEDS[i+1])\n",
        "\n",
        "    diff = v2 - v1\n",
        "    step = diff / STEPS\n",
        "    current = v1.copy()\n",
        "\n",
        "    for j in tqdm(range(STEPS), desc=f\"Seed {SEEDS[i]}\"):\n",
        "      current = current + step\n",
        "      img = generate_image(device, G, current)\n",
        "      img.save(f'./results/frame-{idx}.png')\n",
        "      idx+=1\n",
        "  \n",
        "  # Link the images into a video.\n",
        "  !ffmpeg -r 30 -i /content/results/frame-%d.png -vcodec mpeg4 -y movie.mp4"
      ],
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qsYLbnR8gitF"
      },
      "source": [
        "def download_video():\n",
        "  from google.colab import files\n",
        "  files.download('movie.mp4')"
      ],
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nyn3jHoqHKpP"
      },
      "source": [
        "if __name__== \"__main__\":\n",
        "    device = torch.device('cuda')\n",
        "    G = download_torch_pretrained_instance(device)\n",
        "    get_images(G, device)\n",
        "    explore_seed(G)\n",
        "    morph_video(G, device)\n",
        "    download_video()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}